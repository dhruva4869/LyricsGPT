README generated by AI. I do hope to create an AI in the future, myself that will do this for me 😁 🤖

# Lyrics GPT

A custom implementation of a Generative Pre-trained Transformer (GPT) model specifically designed for generating song lyrics. This project implements the core transformer architecture from scratch using PyTorch, including self-attention mechanisms, multi-head attention, and feed-forward networks.

## 🎵 Features

- **Custom GPT Architecture**: Built from scratch using PyTorch
- **Lyrics Generation**: Trained specifically on song lyrics dataset
- **Modular Design**: Clean separation of components (attention, feed-forward, blocks)
- **Character-level Tokenization**: Simple character-based encoding/decoding
- **Multi-GPU Support**: Automatic device detection (CUDA, MPS, CPU)
- **Training Monitoring**: Built-in loss tracking and early stopping

## 🏗️ Architecture

The model follows the standard GPT architecture with the following components:

### Core Components

1. **SelfAttentionHead** (`self_attention.py`)
   - Implements scaled dot-product attention
   - Includes causal masking for autoregressive generation
   - Dropout for regularization

2. **MultiHeadAttention** (`multi_attention.py`)
   - Combines multiple attention heads
   - Linear projection to maintain embedding dimensions
   - Concatenates outputs from all heads

3. **FeedForward** (`feed_forward.py`)
   - Two-layer MLP with GELU activation
   - Layer normalization between layers
   - Dropout for regularization

4. **Block** (`block.py`)
   - Transformer block combining attention and feed-forward
   - Residual connections with layer normalization
   - Pre-norm architecture

5. **LyricsGPT** (`main.py`)
   - Main model class
   - Token and position embeddings
   - Stack of transformer blocks
   - Language modeling head

## 📊 Model Configuration

```python
# Model Parameters
batch_size = 64
block_size = 256
learning_rate = 2e-4
n_embd = 384          # Embedding dimension
n_head = 4            # Number of attention heads
n_layer = 3           # Number of transformer layers
dropout = 0.2         # Dropout rate

# Training Parameters
max_iterations = 2000
eval_iters = 200
check_progress = 200
```

## 🚀 Getting Started

### Prerequisites

- Python 3.7+
- PyTorch 1.9+
- CUDA (optional, for GPU acceleration)
- MPS (optional, for Apple Silicon acceleration)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd GPT
```

2. Install dependencies:
```bash
pip install torch torchvision torchaudio
```

3. Prepare your dataset:
   - Place your lyrics text file in `datasets/lyrics.txt`
   - The current dataset contains song lyrics in a structured format

### Usage

1. **Training the Model**:
```bash
python main.py
```

The model will automatically:
- Load and preprocess the lyrics dataset
- Split data into train/test sets (90/10)
- Train for 2000 iterations with progress monitoring
- Generate sample lyrics at the end

2. **Generating Lyrics**:
The model generates lyrics by:
- Starting with a context (empty by default)
- Autoregressively predicting the next character
- Using temperature sampling for diversity

## 📁 Project Structure

```
GPT/
├── main.py                 # Main training script and LyricsGPT model
├── self_attention.py       # Self-attention head implementation
├── multi_attention.py      # Multi-head attention implementation
├── block.py               # Transformer block implementation
├── feed_forward.py        # Feed-forward network implementation
├── datasets/
│   └── lyrics.txt         # Training dataset (song lyrics)
├── tiktokenizer/          # Placeholder for custom tokenizer
└── README.md              # This file
```

## 🔧 Customization

### Model Architecture
- Modify `n_embd`, `n_head`, `n_layer` in `main.py` to change model size
- Adjust `block_size` to change context length
- Modify dropout rates for regularization

### Training Parameters
- Change `max_iterations` for training duration
- Adjust `learning_rate` for optimization
- Modify `batch_size` based on available memory

### Tokenization
The current implementation uses character-level tokenization. To improve:
- Implement a custom tokenizer in the `tiktokenizer/` directory
- Replace the simple character encoding with subword tokenization
- This is marked as a TODO in the code

## 📈 Training Process

1. **Data Preprocessing**: Character-level tokenization of lyrics
2. **Model Initialization**: Xavier/Glorot initialization for weights
3. **Training Loop**: 
   - Batch generation from training data
   - Forward pass through transformer blocks
   - Loss computation (cross-entropy)
   - Backpropagation and optimization
4. **Evaluation**: Regular loss estimation on train/test splits
5. **Generation**: Sample new lyrics using trained model

## 🎯 Performance

The model achieves reasonable performance for lyrics generation:
- **Parameters**: ~2M parameters (configurable)
- **Training**: 2000 iterations with early stopping
- **Memory**: Efficient batch processing with configurable batch size
- **Device**: Automatic GPU/CPU detection and utilization

## 🔮 Future Improvements

- [ ] **Custom Tokenizer**: Implement subword tokenization for better efficiency
- [ ] **Data Augmentation**: Techniques to increase dataset diversity
- [ ] **Model Scaling**: Larger models with more parameters
- [ ] **Fine-tuning**: Domain-specific fine-tuning capabilities
- [ ] **Evaluation Metrics**: BLEU, perplexity, and other NLP metrics
- [ ] **Interactive Generation**: Real-time lyrics generation interface

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## 📝 License

This project is open source and available under the [MIT License](LICENSE).

## 🙏 Acknowledgments

- Inspired by the original GPT paper by Radford et al.
- Built using PyTorch framework
- Dataset contains various song lyrics for training

## 📞 Support

For questions or issues, please open an issue on the repository or contact the maintainers.

---

**Note**: This is an educational implementation of GPT for lyrics generation. For production use, consider using established libraries like Hugging Face Transformers or OpenAI's GPT models.
