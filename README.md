README generated by AI. I do hope to create another AI from scratch in the future that will do this for me üòÅ ü§ñ

# Lyrics GPT

A custom implementation of a Generative Pre-trained Transformer (GPT) model specifically designed for generating song lyrics. This project implements the core transformer architecture from scratch using PyTorch, including self-attention mechanisms, multi-head attention, and feed-forward networks.

## üéµ Features

- **Custom GPT Architecture**: Built from scratch using PyTorch
- **Lyrics Generation**: Trained specifically on song lyrics dataset
- **Modular Design**: Clean separation of components (attention, feed-forward, blocks)
- **Multiple Tokenizer Implementations**: 4 different tokenization approaches
  - Advanced BPE tokenizer with OpenAI pattern support
  - Educational BPE implementation for learning
  - Official tiktoken library integration
  - Basic character-level encoding
- **Multi-GPU Support**: Automatic device detection (CUDA, MPS, CPU)
- **Training Monitoring**: Built-in loss tracking and early stopping
- **Tokenizer Visualization**: Tools for analyzing tokenization patterns

## üèóÔ∏è Architecture

The model follows the standard GPT architecture with the following components:

### Core Components

1. **SelfAttentionHead** (`self_attention.py`)
   - Implements scaled dot-product attention
   - Includes causal masking for autoregressive generation
   - Dropout for regularization

2. **MultiHeadAttention** (`multi_attention.py`)
   - Combines multiple attention heads
   - Linear projection to maintain embedding dimensions
   - Concatenates outputs from all heads

3. **FeedForward** (`feed_forward.py`)
   - Two-layer MLP with GELU activation
   - Layer normalization between layers
   - Dropout for regularization

4. **Block** (`block.py`)
   - Transformer block combining attention and feed-forward
   - Residual connections with layer normalization
   - Pre-norm architecture

5. **LyricsGPT** (`main.py`)
   - Main model class
   - Token and position embeddings
   - Stack of transformer blocks
   - Language modeling head

## üî§ Tokenizer Implementations

This project includes **4 different tokenizer implementations** in the `tiktokenizer/` directory:

### 1. **TikToken** (`tiktoken.py`)
A comprehensive BPE (Byte Pair Encoding) tokenizer implementation that supports multiple OpenAI tokenizer patterns:

- **Supported Patterns**:
  - `GPT-2`: Original GPT-2 tokenizer pattern
  - `r50k`: Refined 50k vocabulary pattern
  - `cl100k_base`: GPT-4 tokenizer pattern (100k vocabulary)
  - `o200k_base`: Extended 200k vocabulary pattern

- **Features**:
  - Random pattern selection for training diversity
  - UTF-8 byte-level encoding with BPE merging
  - Configurable vocabulary size
  - Full encode/decode functionality
  - Visualization tools for token analysis
  - Round-trip testing capabilities

- **Usage**:
```python
tokenizer = TikToken(text, vocab_size=500)
tokenizer.train()
tokens = tokenizer.encode("Hello world!")
decoded = tokenizer.decode(tokens)
```

### 2. **NaiveBPE** (`naive_bpe.py`)
A simplified BPE implementation for educational purposes:

- **Features**:
  - Basic byte pair encoding algorithm
  - Character frequency analysis
  - Simple merge operations
  - Configurable pair count limits

- **Implementation**:
  - Inherits from `SimpleBitEncode` for basic functionality
  - Preprocesses text to find most common character pairs
  - Creates new tokens by merging frequent pairs
  - Supports both encoding and decoding operations

### 3. **ActualTikTokenizer** (`actual_tiktokenizer.py`)
A wrapper around the official `tiktoken` library for comparison:

- **Features**:
  - Uses official OpenAI tiktoken library
  - Supports `cl100k_base` encoding (GPT-4 tokenizer)
  - Special token handling (`<|endoftext|>`)
  - Token-by-token analysis and visualization

- **Usage**:
```python
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
tokens = enc.encode(text, allowed_special={"<|endoftext|>"})
```

### 4. **SimpleBitEncode** (`naive_bpe.py`)
A basic character-level encoder:

- **Features**:
  - Simple UTF-8 byte encoding
  - Character-to-integer mapping
  - Basic encode/decode functionality
  - Foundation for more complex tokenizers

## üîß Tokenizer Comparison

| Tokenizer | Type | Vocabulary | Special Features | Use Case |
|-----------|------|------------|------------------|----------|
| **TikToken** | BPE | Configurable | Multi-pattern support, visualization | Production-ready |
| **NaiveBPE** | BPE | Limited | Educational, simple implementation | Learning |
| **ActualTikTokenizer** | Official | 100k+ | Official OpenAI compatibility | Benchmarking |
| **SimpleBitEncode** | Character | 256 | Basic encoding | Foundation |

## üìä Model Configuration

```python
# Model Parameters
batch_size = 64
block_size = 256
learning_rate = 2e-4
n_embd = 384          # Embedding dimension
n_head = 4            # Number of attention heads
n_layer = 3           # Number of transformer layers
dropout = 0.2         # Dropout rate

# Training Parameters
max_iterations = 2000
eval_iters = 200
check_progress = 200
```

## üöÄ Getting Started

### Prerequisites

- Python 3.7+
- PyTorch 1.9+
- CUDA (optional, for GPU acceleration)
- MPS (optional, for Apple Silicon acceleration)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd GPT
```

2. Install dependencies:
```bash
pip install torch torchvision torchaudio
```

3. Prepare your dataset:
   - Place your lyrics text file in `datasets/lyrics.txt`
   - The current dataset contains song lyrics in a structured format

### Usage

1. **Training the Model**:
```bash
python main.py
```

The model will automatically:
- Load and preprocess the lyrics dataset
- Split data into train/test sets (90/10)
- Train for 2000 iterations with progress monitoring
- Generate sample lyrics at the end

2. **Generating Lyrics**:
The model generates lyrics by:
- Starting with a context (empty by default)
- Autoregressively predicting the next character
- Using temperature sampling for diversity

## üìÅ Project Structure

```
GPT/
‚îú‚îÄ‚îÄ main.py                 # Main training script and LyricsGPT model
‚îú‚îÄ‚îÄ self_attention.py       # Self-attention head implementation
‚îú‚îÄ‚îÄ multi_attention.py      # Multi-head attention implementation
‚îú‚îÄ‚îÄ block.py               # Transformer block implementation
‚îú‚îÄ‚îÄ feed_forward.py        # Feed-forward network implementation
‚îú‚îÄ‚îÄ test_improved_bpe.py   # Testing script for BPE tokenizer
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ lyrics.txt         # Training dataset (song lyrics) - generated some songs and reused same over and over for testing
‚îú‚îÄ‚îÄ tiktokenizer/          # Custom tokenizer implementations
‚îÇ   ‚îú‚îÄ‚îÄ tiktoken.py        # TikToken - Comprehensive BPE tokenizer with multiple patterns
‚îÇ   ‚îú‚îÄ‚îÄ naive_bpe.py       # NaiveBPE - Simple BPE implementation for learning
‚îÇ   ‚îî‚îÄ‚îÄ actual_tiktokenizer.py # ActualTikTokenizer - Official tiktoken wrapper
‚îî‚îÄ‚îÄ README.md              # This file
```

## üîß Customization

### Model Architecture
- Modify `n_embd`, `n_head`, `n_layer` in `main.py` to change model size
- Adjust `block_size` to change context length
- Modify dropout rates for regularization

### Training Parameters
- Change `max_iterations` for training duration
- Adjust `learning_rate` for optimization
- Modify `batch_size` based on available memory

### Tokenization
The project includes multiple tokenization options:

**Current Implementation**: Character-level tokenization in the main model

**Available Tokenizers** (in `tiktokenizer/` directory):
- **TikToken**: Production-ready BPE tokenizer with multiple OpenAI patterns
- **NaiveBPE**: Educational BPE implementation for learning
- **ActualTikTokenizer**: Official tiktoken library wrapper for benchmarking
- **SimpleBitEncode**: Basic character-level encoder

**Integration**: To use advanced tokenization:
1. Import desired tokenizer from `tiktokenizer/`
2. Replace character encoding in `main.py` with tokenizer.encode()
3. Update vocabulary size and embedding dimensions accordingly

## üìà Training Process

1. **Data Preprocessing**: Character-level tokenization of lyrics
2. **Model Initialization**: Xavier/Glorot initialization for weights
3. **Training Loop**: 
   - Batch generation from training data
   - Forward pass through transformer blocks
   - Loss computation (cross-entropy)
   - Backpropagation and optimization
4. **Evaluation**: Regular loss estimation on train/test splits
5. **Generation**: Sample new lyrics using trained model

## üéØ Performance

The model achieves reasonable performance for lyrics generation:
- **Parameters**: ~2M parameters (configurable)
- **Training**: 2000 iterations with early stopping
- **Memory**: Efficient batch processing with configurable batch size
- **Device**: Automatic GPU/CPU detection and utilization

## üîÆ Future Improvements

- [x] **Custom Tokenizer**: ‚úÖ Implemented 4 different tokenizer approaches
- [x] **Tokenizer Integration**: Integrate advanced tokenizers into main model
- [ ] **Data Augmentation**: Techniques to increase dataset diversity
- [ ] **Model Scaling**: Larger models with more parameters
- [ ] **Fine-tuning**: Domain-specific fine-tuning capabilities
- [ ] **Evaluation Metrics**: BLEU, perplexity, and other NLP metrics
- [ ] **Interactive Generation**: Real-time lyrics generation interface
- [ ] **Tokenizer Benchmarking**: Compare performance across different tokenizers

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìù License

This project is open source and available under the [MIT License](LICENSE).

## üôè Acknowledgments

- Inspired by the original GPT paper by Radford et al.
- Attention is all you need, you have read this before as well üòÅ
- Built using PyTorch framework
- Dataset contains various song lyrics for training

## üìû Support

For questions or issues, please open an issue on the repository or contact the maintainers.

---

**Note**: This is an educational implementation of GPT for lyrics generation. For production use, consider using established libraries like Hugging Face Transformers or OpenAI's GPT models.
